---
title: "On the training efficiency of shallow architectures for physics informed neural networks"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: 'This paper is about Physics-informed Neural Networks (PINNs), which are neural models trained by minimizing a combination of the residual of the governing partial differential equation and the initial and boundary conditions. Although PINNs have demonstrated success, the impact of architecture depth on training efficiency is not well-documented. The paper shows that, for a given model size, shallow networks converge faster than deeper networks for the same error characteristics. The study uses the 1D Poisson equation to analyze the gradient behavior of residual and boundary loss terms, demonstrating that shallow architectures lead to faster convergence. Experimental results support the theoretical findings.'
date: 29-06-2024
venue: 'International Conference on Computational Science 2024'
paperurl: 'https://link.springer.com/chapter/10.1007/978-3-031-63759-9_39'
citation: 'Rishi, J., Gafoor, A., Kumar, S., Subramani, D. (2024). On the Training Efficiency of Shallow Architectures for Physics Informed Neural Networks. In: Franco, L., de Mulatier, C., Paszynski, M., Krzhizhanovskaya, V.V., Dongarra, J.J., Sloot, P.M.A. (eds) Computational Science â€“ ICCS 2024. ICCS 2024. Lecture Notes in Computer Science, vol 14834. Springer, Cham. https://doi.org/10.1007/978-3-031-63759-9_39'

[Download paper here](https://link.springer.com/chapter/10.1007/978-3-031-63759-9_39)
